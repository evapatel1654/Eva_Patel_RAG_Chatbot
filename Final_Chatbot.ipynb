{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:01:38.329157Z",
     "iopub.status.busy": "2024-06-13T12:01:38.328806Z",
     "iopub.status.idle": "2024-06-13T12:03:08.870715Z",
     "shell.execute_reply": "2024-06-13T12:03:08.869649Z",
     "shell.execute_reply.started": "2024-06-13T12:01:38.329128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - pytorch\n",
      " - rapidsai\n",
      " - nvidia\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - faiss-cpu=1.8.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2024.6.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
      "    faiss-cpu-1.8.0            |py3.10_h9d89a2e_0_cpu         4.4 MB  pytorch\n",
      "    libfaiss-1.8.0             |   hf65b397_0_cpu         5.2 MB  pytorch\n",
      "    openssl-3.3.1              |       h4ab18f5_0         2.8 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        12.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  faiss-cpu          pytorch/linux-64::faiss-cpu-1.8.0-py3.10_h9d89a2e_0_cpu \n",
      "  libfaiss           pytorch/linux-64::libfaiss-1.8.0-hf65b397_0_cpu \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  certifi                             2024.2.2-pyhd8ed1ab_0 --> 2024.6.2-pyhd8ed1ab_0 \n",
      "  openssl                                  3.3.0-h4ab18f5_3 --> 3.3.1-h4ab18f5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "libfaiss-1.8.0       | 5.2 MB    |                                       |   0% \n",
      "faiss-cpu-1.8.0      | 4.4 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "openssl-3.3.1        | 2.8 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.6.2     | 157 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.6.2     | 157 KB    | #######5                              |  20% \u001b[A\u001b[A\u001b[A\n",
      "faiss-cpu-1.8.0      | 4.4 MB    | 1                                     |   0% \u001b[A\n",
      "\n",
      "libfaiss-1.8.0       | 5.2 MB    | 1                                     |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2024.6.2     | 157 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "libfaiss-1.8.0       | 5.2 MB    | ############################3         |  77% \u001b[A\n",
      "\n",
      "openssl-3.3.1        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "openssl-3.3.1        | 2.8 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pytorch faiss-cpu=1.8.0 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:04:04.113556Z",
     "iopub.status.busy": "2024-06-13T12:04:04.113159Z",
     "iopub.status.idle": "2024-06-13T12:04:44.615277Z",
     "shell.execute_reply": "2024-06-13T12:04:44.614145Z",
     "shell.execute_reply.started": "2024-06-13T12:04:04.113523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.5 PyMuPDFb-1.24.3\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install transformers torch\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:04:51.640833Z",
     "iopub.status.busy": "2024-06-13T12:04:51.640062Z",
     "iopub.status.idle": "2024-06-13T12:05:35.617006Z",
     "shell.execute_reply": "2024-06-13T12:05:35.615998Z",
     "shell.execute_reply.started": "2024-06-13T12:04:51.640797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e4cfa866a94a52a3529a620ee2d696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c5c0e6d774fa1a15426b5d0229233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_encoder_tokenizer/tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391c927ccce54607a27e807943689a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "question_encoder_tokenizer/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a90dc1daab345d0b33011e373bf1c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ncoder_tokenizer/special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dc00e457c14038973595da955d3b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)enerator_tokenizer/tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e8dae94cd544e1b3e998b70e053df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generator_tokenizer/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b655a06780e4811800a201a84aba5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generator_tokenizer/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccc61b197ad4696bcdb0d1a7d16bcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)erator_tokenizer/special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b16fa2001143c68fd3a6b79da3c6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7688678399344dabb3cccf229e6094af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a427aa1c7be242b8bf34928707acacad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/40.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ace49859fe487a9bc62edac866a999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a385e17e0b57424eb0031e2bd3ce8b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7abf52df7854fa89d29682d0f3601c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6a1556ac7e49b3b9993883842cea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at facebook/rag-token-base were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "rag_retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "rag_token_generator = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-base\", retriever=rag_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:05:40.587302Z",
     "iopub.status.busy": "2024-06-13T12:05:40.586733Z",
     "iopub.status.idle": "2024-06-13T12:05:40.846100Z",
     "shell.execute_reply": "2024-06-13T12:05:40.845362Z",
     "shell.execute_reply.started": "2024-06-13T12:05:40.587272Z"
    }
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = '/kaggle/input/dataset/dataset.pdf'\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:05:42.874759Z",
     "iopub.status.busy": "2024-06-13T12:05:42.873920Z",
     "iopub.status.idle": "2024-06-13T12:05:42.883264Z",
     "shell.execute_reply": "2024-06-13T12:05:42.882193Z",
     "shell.execute_reply.started": "2024-06-13T12:05:42.874727Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_text(text, max_length=1024):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text(pdf_text, max_length=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:05:44.607882Z",
     "iopub.status.busy": "2024-06-13T12:05:44.606940Z",
     "iopub.status.idle": "2024-06-13T12:13:44.185603Z",
     "shell.execute_reply": "2024-06-13T12:13:44.184550Z",
     "shell.execute_reply.started": "2024-06-13T12:05:44.607847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 12:05:46.121579: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-13 12:05:46.121691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-13 12:05:46.225826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fd3d9c7d7d49978a3935963713e9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bb15ce834c45339d8402717aa3eb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3446ac7f804cd59f04e6a04e00baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f46586c209489c93d7ed991b0f20c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b209a222924c3496734d345d293967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc540c3059443c2895f035b4b056dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=tokenizer)\n",
    "\n",
    "def split_text_for_summarization(text, max_length=1024):\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "\n",
    "    for word in words:\n",
    "        current_segment.append(word)\n",
    "        if len(tokenizer.encode(' '.join(current_segment), truncation=False)) >= max_length:\n",
    "            current_segment.pop()\n",
    "            segments.append(' '.join(current_segment))\n",
    "            current_segment = [word]\n",
    "\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "\n",
    "    return segments\n",
    "\n",
    "def summarize_text(text, max_length=150):\n",
    "    segments = split_text_for_summarization(text)\n",
    "    summaries = []\n",
    "    for segment in segments:\n",
    "        summary = summarizer(segment, max_length=max_length, min_length=50, do_sample=False)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "    combined_summary = ' '.join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "summarized_chunks = [summarize_text(chunk) for chunk in text_chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:13:51.627779Z",
     "iopub.status.busy": "2024-06-13T12:13:51.627010Z",
     "iopub.status.idle": "2024-06-13T12:14:29.920699Z",
     "shell.execute_reply": "2024-06-13T12:14:29.919408Z",
     "shell.execute_reply.started": "2024-06-13T12:13:51.627744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c599b4281f47828eee3d5a772ee56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536b21f910194192b9dc622b1dbf66ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2a72651d074b7abc691cc5e2d6cf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/15.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e70c17bcda14f06b70c92c852d4f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10ba01b2ee74ee9a163a3e2d83de1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d44dcfcebb1473b906d12f9cf276bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "qg_tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "qg_model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def generate_questions(summary):\n",
    "    input_text = f\"generate questions: {summary}\"\n",
    "    input_ids = qg_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output = qg_model.generate(input_ids)\n",
    "    questions = qg_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return questions.split(\"? \")  # Split the generated questions by ?\n",
    "\n",
    "\n",
    "questions = generate_questions(summarized_chunks[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:14:33.015355Z",
     "iopub.status.busy": "2024-06-13T12:14:33.014722Z",
     "iopub.status.idle": "2024-06-13T12:14:56.927358Z",
     "shell.execute_reply": "2024-06-13T12:14:56.926239Z",
     "shell.execute_reply.started": "2024-06-13T12:14:33.015320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917de47ee1ed4a4386e2249cf29fe033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8162e402e515484c86fa486d5004fcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af043a2d27564a65add2a9e72c78d1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a41f40c80d4858bdfc1ae56ddac654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b95450b775d4c4aafea3f4f74fa7157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512475c807c04b03a83377791205eb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def generate_qa_pairs(summarized_chunks):\n",
    "    qa_pairs = []\n",
    "    for summary in summarized_chunks:\n",
    "        questions = generate_questions(summary)\n",
    "        for question in questions:\n",
    "            if question.strip():\n",
    "                q = {\"question\": question + \"?\", \"context\": summary}\n",
    "                answer = qa_pipeline(question=q['question'], context=q['context'])\n",
    "                qa_pairs.append({\"query\": q['question'], \"response\": answer['answer']})\n",
    "    return qa_pairs\n",
    "\n",
    "dataset = generate_qa_pairs(summarized_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:15:55.734878Z",
     "iopub.status.busy": "2024-06-13T12:15:55.734227Z",
     "iopub.status.idle": "2024-06-13T12:15:55.742440Z",
     "shell.execute_reply": "2024-06-13T12:15:55.741476Z",
     "shell.execute_reply.started": "2024-06-13T12:15:55.734847Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    rag_token_generator.to(device)\n",
    "\n",
    "\n",
    "    input_dict = rag_tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    input_ids = input_dict[\"input_ids\"]\n",
    "    attention_mask = input_dict[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        question_hidden_states = rag_token_generator.question_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "        question_hidden_states = question_hidden_states.to(device)\n",
    "\n",
    "        retriever_results = rag_retriever(question_hidden_states)\n",
    "        context_input_ids = retriever_results[\"context_input_ids\"].to(device)\n",
    "        context_attention_mask = retriever_results[\"context_attention_mask\"].to(device)\n",
    "\n",
    "        generated_outputs = rag_token_generator.generate(input_ids=input_ids, context_input_ids=context_input_ids, context_attention_mask=context_attention_mask)\n",
    "\n",
    "    response = rag_tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:15:59.862651Z",
     "iopub.status.busy": "2024-06-13T12:15:59.862246Z",
     "iopub.status.idle": "2024-06-13T12:15:59.869569Z",
     "shell.execute_reply": "2024-06-13T12:15:59.868491Z",
     "shell.execute_reply.started": "2024-06-13T12:15:59.862619Z"
    }
   },
   "outputs": [],
   "source": [
    "def chatbot(dataset):\n",
    "    print(\"Welcome to the PDF Summary Chatbot!\")\n",
    "    print(\"Type 'exit' to end the conversation.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting the chatbot. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response_found = False\n",
    "        for qa_pair in dataset:\n",
    "            if user_input.lower() in qa_pair['query'].lower():\n",
    "                print(\"Chatbot:\", qa_pair['response'])\n",
    "                response_found = True\n",
    "                break\n",
    "        \n",
    "        if not response_found:\n",
    "            response = generate_response(user_input)\n",
    "            print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:16:01.935714Z",
     "iopub.status.busy": "2024-06-13T12:16:01.934614Z",
     "iopub.status.idle": "2024-06-13T12:18:06.221687Z",
     "shell.execute_reply": "2024-06-13T12:18:06.220611Z",
     "shell.execute_reply.started": "2024-06-13T12:16:01.935673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the PDF Summary Chatbot!\n",
      "Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How does the insurer assess the value of my vehicle?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: The insurer assesses the value of your vehicle based on factors such as its age, make and model, condition, mileage, and market value at the time of the claim.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How can I contact customer support?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: You can contact customer support by calling the helpline number provided in your policy document, or by visiting the insurer's website for additional contact options.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the number for Motor Legal Cover Help?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: 0345 877 6680\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Can I add additional drivers to my policy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Yes, you can add additional drivers to your policy. You will need to provide their details, and the premium may be adjusted based on their driving history and other factors.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What are the policyholder's responsibilities?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: The policyholder's responsibilities include providing accurate information, paying premiums on time, and notifying the insurer of any changes that may affect the coverage.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Are personal belongings covered under this policy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Personal belongings may be covered under the policy, but there are limitations and exclusions. Check the policy document for specific details on coverage limits and excluded items.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chatbot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:18:36.557018Z",
     "iopub.status.busy": "2024-06-13T12:18:36.556635Z",
     "iopub.status.idle": "2024-06-13T12:18:51.836603Z",
     "shell.execute_reply": "2024-06-13T12:18:51.835432Z",
     "shell.execute_reply.started": "2024-06-13T12:18:36.556987Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=ca919b84ca9f951c71cc770c4d1f17a02a44d1fb65b392a5fdcb9549c987bac3\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T12:30:35.268556Z",
     "iopub.status.busy": "2024-06-13T12:30:35.268157Z",
     "iopub.status.idle": "2024-06-13T12:30:39.950926Z",
     "shell.execute_reply": "2024-06-13T12:30:39.950068Z",
     "shell.execute_reply.started": "2024-06-13T12:30:35.268528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.08714335918223264\n",
      "Average ROUGE-1: 0.09003837600972746, ROUGE-2: 0.0744632886045723, ROUGE-L: 0.09003837600972746\n",
      "QA Accuracy: 28.57142857142857%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    references = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "    return corpus_bleu(references, [candidate], smoothing_function=smoothing_function)\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    rouge_1 = scores['rouge1'].fmeasure\n",
    "    rouge_2 = scores['rouge2'].fmeasure\n",
    "    rouge_l = scores['rougeL'].fmeasure\n",
    "    return rouge_1, rouge_2, rouge_l\n",
    "\n",
    "def evaluate_qa_accuracy(dataset):\n",
    "    total_questions = len(dataset)\n",
    "    correct_answers = 0\n",
    "    \n",
    "    for qa_pair in dataset:\n",
    "        question = qa_pair['query']\n",
    "        expected_answer = qa_pair['response']\n",
    "        \n",
    "\n",
    "        answer = qa_pipeline(question=question, context=expected_answer)\n",
    "        chatbot_answer = answer['answer']\n",
    "        \n",
    "\n",
    "        if chatbot_answer.strip() == expected_answer.strip():\n",
    "            correct_answers += 1\n",
    "    \n",
    "    accuracy = (correct_answers / total_questions) * 100\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_summary_quality(dataset, summarized_chunks):\n",
    "    total_bleu_score = 0.0\n",
    "    total_rouge_1 = 0.0\n",
    "    total_rouge_2 = 0.0\n",
    "    total_rouge_l = 0.0\n",
    "    \n",
    "    for i, segment in enumerate(summarized_chunks):\n",
    "        reference_summary = dataset[i]['response']\n",
    "        candidate_summary = segment\n",
    "        \n",
    "\n",
    "        bleu_score = calculate_bleu(reference_summary, candidate_summary)\n",
    "        total_bleu_score += bleu_score\n",
    "        \n",
    "\n",
    "        rouge_1, rouge_2, rouge_l = calculate_rouge(reference_summary, candidate_summary)\n",
    "        total_rouge_1 += rouge_1\n",
    "        total_rouge_2 += rouge_2\n",
    "        total_rouge_l += rouge_l\n",
    "    \n",
    "    num_segments = len(summarized_chunks)\n",
    "    avg_bleu_score = total_bleu_score / num_segments\n",
    "    avg_rouge_1 = total_rouge_1 / num_segments\n",
    "    avg_rouge_2 = total_rouge_2 / num_segments\n",
    "    avg_rouge_l = total_rouge_l / num_segments\n",
    "    \n",
    "    return avg_bleu_score, avg_rouge_1, avg_rouge_2, avg_rouge_l\n",
    "\n",
    "avg_bleu, avg_rouge_1, avg_rouge_2, avg_rouge_l = evaluate_summary_quality(dataset, summarized_chunks)\n",
    "print(f\"Average BLEU Score: {avg_bleu}\")\n",
    "print(f\"Average ROUGE-1: {avg_rouge_1}, ROUGE-2: {avg_rouge_2}, ROUGE-L: {avg_rouge_l}\")\n",
    "\n",
    "qa_accuracy = evaluate_qa_accuracy(dataset)\n",
    "print(f\"QA Accuracy: {qa_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5197686,
     "sourceId": 8672394,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5205153,
     "sourceId": 8682429,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
